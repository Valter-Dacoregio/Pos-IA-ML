{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83888599-a4d4-490a-b311-07fe1e3e0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O PySpark suporta dois formatos principais de arquivos JSON:\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# 1. JSON de linha única (single-line JSON), onde todo o conteúdo está dentro de uma lista ([ ... ]). Esse formato é geralmente usado para pequenos arquivos JSON.\n",
    "example_data = [\n",
    "    {\"id\": 1, \"nome\": \"Valter\", \"idade\": 28, \"cidade\": \"Sao Paulo\"},\n",
    "    {\"id\": 2, \"nome\": \"Ana\", \"idade\": 34, \"cidade\": \"Rio de Janeiro\"},\n",
    "    {\"id\": 3, \"nome\": \"Carlos\", \"idade\": 25, \"cidade\": \"Belo Horizonte\"},\n",
    "    {\"id\": 4, \"nome\": \"Fernanda\", \"idade\": 40, \"cidade\": \"Porto Alegre\"},\n",
    "    {\"id\": 5, \"nome\": \"Roberto\", \"idade\": 31, \"cidade\": \"Curitiba\"}\n",
    "]\n",
    "# Salvar um JSON de teste\n",
    "with open(\"clientes_single_line.json\", \"w\") as file:\n",
    "    json.dump(example_data, file)\n",
    "\n",
    "\n",
    "# 2. JSON de linha (line-delimited JSON), onde cada objeto JSON está em uma linha separada. \n",
    "# Esse formato é mais eficiente para grandes conjuntos de dados e é o padrão usado pelo PySpark para leitura de JSON.\n",
    "# Dados\n",
    "data = [\n",
    "    {\"id\": 1, \"nome\": \"Valter\", \"idade\": 28, \"cidade\": \"Sao Paulo\"},\n",
    "    {\"id\": 2, \"nome\": \"Ana\", \"idade\": 34, \"cidade\": \"Rio de Janeiro\"},\n",
    "    {\"id\": 3, \"nome\": \"Carlos\", \"idade\": 25, \"cidade\": \"Belo Horizonte\"},\n",
    "    {\"id\": 4, \"nome\": \"Fernanda\", \"idade\": 40, \"cidade\": \"Porto Alegre\"},\n",
    "    {\"id\": 5, \"nome\": \"Roberto\", \"idade\": 31, \"cidade\": \"Curitiba\"}\n",
    "]\n",
    "# Salvar em JSON de linha\n",
    "with open(\"clientes_line_delimited.json\", \"w\") as file:\n",
    "    for record in data:\n",
    "        file.write(json.dumps(record) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6de8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb49d298-d3f3-455a-846c-a4e3993d2899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/24 09:23:28 WARN Utils: Your hostname, Mac-mini-de-Valter.local resolves to a loopback address: 127.0.0.1; using 192.168.0.100 instead (on interface en0)\n",
      "24/11/24 09:23:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/24 09:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Iniciar uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc4c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+------+\n",
      "| id|        produto|quantidade| valor|\n",
      "+---+---------------+----------+------+\n",
      "|  1|       Notebook|         2|2500.0|\n",
      "|  2|     Smartphone|         5|1000.0|\n",
      "|  3|         Tablet|         3| 800.0|\n",
      "|  4|Fones de ouvido|        10| 150.0|\n",
      "|  5|     Smartwatch|         4| 500.0|\n",
      "+---+---------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo CSV\n",
    "df_vendas = spark.read.csv(\"vendas.csv\", header=True, inferSchema=True)\n",
    "df_vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10efbd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-----+--------+\n",
      "|        cidade| id|idade|    nome|\n",
      "+--------------+---+-----+--------+\n",
      "|     Sao Paulo|  1|   28|  Valter|\n",
      "|Rio de Janeiro|  2|   34|     Ana|\n",
      "|Belo Horizonte|  3|   25|  Carlos|\n",
      "|  Porto Alegre|  4|   40|Fernanda|\n",
      "|      Curitiba|  5|   31| Roberto|\n",
      "+--------------+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo JSON\n",
    "df_single_line = spark.read.json(\"clientes_single_line.json\")\n",
    "df_single_line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65de8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-----+--------+\n",
      "|        cidade| id|idade|    nome|\n",
      "+--------------+---+-----+--------+\n",
      "|     Sao Paulo|  1|   28|  Valter|\n",
      "|Rio de Janeiro|  2|   34|     Ana|\n",
      "|Belo Horizonte|  3|   25|  Carlos|\n",
      "|  Porto Alegre|  4|   40|Fernanda|\n",
      "|      Curitiba|  5|   31| Roberto|\n",
      "+--------------+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tente ler o novo JSON\n",
    "df_line_delimited = spark.read.json(\"clientes_line_delimited.json\")\n",
    "df_line_delimited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37afe30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema do DataFrame de Vendas:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- produto: string (nullable = true)\n",
      " |-- quantidade: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      "\n",
      "Esquema do DataFrame de Clientes:\n",
      "root\n",
      " |-- cidade: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- idade: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      "\n",
      "Dados do DataFrame de Vendas:\n",
      "+---+---------------+----------+------+\n",
      "| id|        produto|quantidade| valor|\n",
      "+---+---------------+----------+------+\n",
      "|  1|       Notebook|         2|2500.0|\n",
      "|  2|     Smartphone|         5|1000.0|\n",
      "|  3|         Tablet|         3| 800.0|\n",
      "|  4|Fones de ouvido|        10| 150.0|\n",
      "|  5|     Smartwatch|         4| 500.0|\n",
      "+---+---------------+----------+------+\n",
      "\n",
      "Dados do DataFrame de Clientes:\n",
      "+--------------+---+-----+--------+\n",
      "|        cidade| id|idade|    nome|\n",
      "+--------------+---+-----+--------+\n",
      "|     Sao Paulo|  1|   28|  Valter|\n",
      "|Rio de Janeiro|  2|   34|     Ana|\n",
      "|Belo Horizonte|  3|   25|  Carlos|\n",
      "|  Porto Alegre|  4|   40|Fernanda|\n",
      "|      Curitiba|  5|   31| Roberto|\n",
      "+--------------+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir os esquemas dos DataFrames\n",
    "print(\"Esquema do DataFrame de Vendas:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "print(\"Esquema do DataFrame de Clientes:\")\n",
    "df_line_delimited.printSchema()\n",
    "\n",
    "# Exibição dos dados:\n",
    "print(\"Dados do DataFrame de Vendas:\")\n",
    "df_vendas.show()\n",
    "\n",
    "print(\"Dados do DataFrame de Clientes:\")\n",
    "df_line_delimited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a99d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vendas com quantidade maior que 3:\n",
      "+---+---------------+----------+------+\n",
      "| id|        produto|quantidade| valor|\n",
      "+---+---------------+----------+------+\n",
      "|  2|     Smartphone|         5|1000.0|\n",
      "|  4|Fones de ouvido|        10| 150.0|\n",
      "|  5|     Smartwatch|         4| 500.0|\n",
      "+---+---------------+----------+------+\n",
      "\n",
      "Clientes de Sao Paulo:\n",
      "+---------+---+-----+------+\n",
      "|   cidade| id|idade|  nome|\n",
      "+---------+---+-----+------+\n",
      "|Sao Paulo|  1|   28|Valter|\n",
      "+---------+---+-----+------+\n",
      "\n",
      "Valor total de vendas:\n",
      "+-------------------------+\n",
      "|sum((valor * quantidade))|\n",
      "+-------------------------+\n",
      "|                  15900.0|\n",
      "+-------------------------+\n",
      "\n",
      "Idade média dos clientes:\n",
      "+----------+\n",
      "|avg(idade)|\n",
      "+----------+\n",
      "|      31.6|\n",
      "+----------+\n",
      "\n",
      "Vendas unidas:\n",
      "+---+---------------+----------+------+\n",
      "| id|        produto|quantidade| valor|\n",
      "+---+---------------+----------+------+\n",
      "|  1|       Notebook|         2|2500.0|\n",
      "|  2|     Smartphone|         5|1000.0|\n",
      "|  3|         Tablet|         3| 800.0|\n",
      "|  4|Fones de ouvido|        10| 150.0|\n",
      "|  5|     Smartwatch|         4| 500.0|\n",
      "|  6|        Monitor|         2| 800.0|\n",
      "|  7|        Teclado|         5| 100.0|\n",
      "|  8|          Mouse|         3|  50.0|\n",
      "+---+---------------+----------+------+\n",
      "\n",
      "None\n",
      "Arquivo Exportados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "#5. Filtragem:\n",
    "# Filtrar vendas com quantidade maior que 3\n",
    "df_vendas_filtrado = df_vendas.filter(col(\"quantidade\") > 3)\n",
    "print(\"Vendas com quantidade maior que 3:\")\n",
    "df_vendas_filtrado.show()\n",
    "\n",
    "# Filtrar clientes de Sao Paulo\n",
    "df_clientes_filtrado = df_line_delimited.filter(col(\"cidade\") == \"Sao Paulo\")\n",
    "print(\"Clientes de Sao Paulo:\")\n",
    "df_clientes_filtrado.show()\n",
    "\n",
    "#6. Agregação:\n",
    "# Calcular o valor total de vendas\n",
    "valor_total = df_vendas.agg(sum(df_vendas[\"valor\"] * df_vendas[\"quantidade\"])).alias(\"valor_total\")\n",
    "print(\"Valor total de vendas:\")\n",
    "valor_total.show()\n",
    "\n",
    "# Calcular a idade média dos clientes\n",
    "idade_media = df_line_delimited.agg(avg(df_line_delimited[\"idade\"])).alias(\"idade_media\")\n",
    "print(\"Idade média dos clientes:\")\n",
    "idade_media.show()\n",
    "\n",
    "# União:\n",
    "# Criar um novo DataFrame com informações adicionais de vendas\n",
    "df_vendas_adicionais = spark.createDataFrame([\n",
    "    (6, \"Monitor\", 2, 800.00),\n",
    "    (7, \"Teclado\", 5, 100.00),\n",
    "    (8, \"Mouse\", 3, 50.00),\n",
    "], [\"id\", \"produto\", \"quantidade\", \"valor\"])\n",
    "\n",
    "# Unir os DataFrames de vendas\n",
    "df_vendas_uniao = df_vendas.union(df_vendas_adicionais)\n",
    "print(\"Vendas unidas:\")\n",
    "print(df_vendas_uniao.show())\n",
    "\n",
    "# Escrita de dados:\n",
    "\n",
    "# Escrever o DataFrame unificado em um arquivo CSV\n",
    "df_vendas_uniao.write.csv(\"exportacao_vendas.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Escrever o DataFrame de clientes filtrado em um arquivo JSON\n",
    "df_clientes_filtrado.write.json(\"exportacao_clientes.json\", mode=\"overwrite\")\n",
    "\n",
    "print(\"Arquivo Exportados com sucesso!\")\n",
    "\n",
    "# Parar a sessão Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
