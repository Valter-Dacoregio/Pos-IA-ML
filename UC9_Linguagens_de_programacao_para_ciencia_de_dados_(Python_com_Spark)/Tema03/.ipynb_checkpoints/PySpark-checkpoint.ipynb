{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83888599-a4d4-490a-b311-07fe1e3e0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb49d298-d3f3-455a-846c-a4e3993d2899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+------+\n",
      "| id|        produto|quantidade| valor|\n",
      "+---+---------------+----------+------+\n",
      "|  1|       Notebook|         2|2500.0|\n",
      "|  2|     Smartphone|         5|1000.0|\n",
      "|  3|         Tablet|         3| 800.0|\n",
      "|  4|Fones de ouvido|        10| 150.0|\n",
      "|  5|     Smartwatch|         4| 500.0|\n",
      "+---+---------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iniciar uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "# Leitura do arquivo CSV\n",
    "df_vendas = spark.read.csv(\"vendas.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "530c7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O PySpark suporta dois formatos principais de arquivos JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-----+------+\n",
      "|        cidade| id|idade|  nome|\n",
      "+--------------+---+-----+------+\n",
      "|     São Paulo|  1|   28|Valter|\n",
      "|Rio de Janeiro|  2|   34|   Ana|\n",
      "+--------------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. JSON de linha única (single-line JSON), onde todo o conteúdo está dentro de uma lista ([ ... ]). Esse formato é geralmente usado para pequenos arquivos JSON.\n",
    "\n",
    "example_data = [\n",
    "    {\"id\": 1, \"nome\": \"Valter\", \"idade\": 28, \"cidade\": \"São Paulo\"},\n",
    "    {\"id\": 2, \"nome\": \"Ana\", \"idade\": 34, \"cidade\": \"Rio de Janeiro\"}\n",
    "]\n",
    "\n",
    "import json\n",
    "\n",
    "# Salvar um JSON de teste\n",
    "with open(\"clientes_single_line.json\", \"w\") as file:\n",
    "    json.dump(example_data, file)\n",
    "\n",
    "# Tente ler o novo JSON\n",
    "df_teste = spark.read.json(\"clientes_single_line.json\")\n",
    "df_teste.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f445f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. JSON de linha (line-delimited JSON), onde cada objeto JSON está em uma linha separada. \n",
    "# Esse formato é mais eficiente para grandes conjuntos de dados e é o padrão usado pelo PySpark para leitura de JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f493925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"clientes.json\", \"r\") as file:\n",
    "#     print(file.read())\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"idade\", IntegerType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Ler JSON com esquema explícito\n",
    "# df_clientes = spark.read.schema(schema).json(\"clientes.json\")\n",
    "\n",
    "df_clientes = spark.read.json(\"clientes.json\")\n",
    "\n",
    "df_clientes.describe()\n",
    "\n",
    "# df_clientes = spark.read.option(\"mode\", \"PERMISSIVE\").json(\"clientes.json\")\n",
    "# df_clientes.select(\"_corrupt_record\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca23a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Dados\n",
    "data = [\n",
    "    {\"id\": 1, \"nome\": \"Valter\", \"idade\": 28, \"cidade\": \"Sao Paulo\"},\n",
    "    {\"id\": 2, \"nome\": \"Ana\", \"idade\": 34, \"cidade\": \"Rio de Janeiro\"},\n",
    "    {\"id\": 3, \"nome\": \"Carlos\", \"idade\": 25, \"cidade\": \"Belo Horizonte\"},\n",
    "    {\"id\": 4, \"nome\": \"Fernanda\", \"idade\": 40, \"cidade\": \"Porto Alegre\"},\n",
    "    {\"id\": 5, \"nome\": \"Roberto\", \"idade\": 31, \"cidade\": \"Curitiba\"}\n",
    "]\n",
    "\n",
    "# Salvar em JSON de linha\n",
    "with open(\"clientes_line.json\", \"w\") as file:\n",
    "    for record in data:\n",
    "        file.write(json.dumps(record) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
